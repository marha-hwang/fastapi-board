from fastapi import FastAPI, Request, status
from .config import server_config
from fastapi import APIRouter
from app.routes import auth_router, user_router, post_router, comment_router, file_router
import logging
from fastapi.exceptions import RequestValidationError
import app.schema.common_schema as common_schema
from fastapi.responses import JSONResponse
from app.core.exception import CustomException, ErrorCode
import os
from fastapi.staticfiles import StaticFiles
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from openai import AsyncOpenAI

app = FastAPI(title="FastAPI + Poetry AI Server")

logging.basicConfig(
    level=logging.INFO,  # ë¡œê·¸ ë ˆë²¨ì„ INFOë¡œ ì„¤ì •
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)


# 1. ì‹¤ì œ ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë” ì´ë¦„
UPLOAD_DIR = "images"
# (í´ë”ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ë‚˜ë‹ˆê¹Œ ì•ˆì „í•˜ê²Œ ìƒì„±)
if not os.path.exists(UPLOAD_DIR):
    os.makedirs(UPLOAD_DIR)
# 2. mount ì„¤ì • (í•µì‹¬ ì½”ë“œ)
app.mount("/images", StaticFiles(directory=UPLOAD_DIR), name="images")

# ê¸°ë³¸ ì—ëŸ¬ì²˜ë¦¬
@app.exception_handler(Exception)
async def default_exception_handler(request: Request, exc: Exception):
    error_msg = str(exc)
    print(f"ğŸš¨ 500 Internal Server Error: {error_msg}")

    response = common_schema.ApiResponse(success=False, 
                                         code=ErrorCode.INTERNAL_SERVER_ERROR, 
                                         message="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content=response.model_dump()
    )

# ìš”ì²­ë°ì´í„°ê°€ pydanticê²€ì¦ì— ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    print(str(exc.errors))
    error_msg = str(exc)
    print(f" Error: {error_msg}")
    for error in exc.errors():
        msg = error["msg"].replace("Value error, ", "")

    response = common_schema.ApiResponse(success=False, 
                                         code=ErrorCode.INVALID_INPUT_VALUE, 
                                         message=msg)
    return JSONResponse(
        status_code=status.HTTP_400_BAD_REQUEST,
        content=response.model_dump()
    )

@app.exception_handler(CustomException)
async def custom_exception_handler(request: Request, exc: CustomException):
    error_msg = str(exc)
    print(f" Error: {error_msg}")
    response = common_schema.ApiResponse(success=False, 
                                         code=exc.code, 
                                         message=exc.message)
    return JSONResponse(
        status_code=status.HTTP_404_NOT_FOUND,
        content=response.model_dump()
    )

@app.get("/")
async def root():
    return {"message": "AI Model Server is running ğŸš€"}

################################### llmëª¨ë¸ api ###################################
# 1. vLLM ì„œë²„ ì—°ê²° ì„¤ì •
public_url = "https://unresumed-maya-hyperaccurately.ngrok-free.dev"
model_name = "Qwen/Qwen2.5-32B-Instruct-AWQ"

VLLM_API_URL = f"{public_url}/v1" 
client = AsyncOpenAI(base_url=VLLM_API_URL, api_key="EMPTY")

class ChatRequest(BaseModel):
    message: str = "ì• êµ­ê°€ ê°€ì‚¬ë¥¼ ì•Œë ¤ì¤˜"
    model: str = model_name

# 2. ìŠ¤íŠ¸ë¦¼ ì œë„ˆë ˆì´í„° í•¨ìˆ˜ (í•µì‹¬)
async def stream_generator(prompt: str, model: str):
    # vLLMì— ìš”ì²­ (stream=True í•„ìˆ˜!)
    stream = await client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        stream=True,  # <--- ì´ê²Œ ì¼œì ¸ ìˆì–´ì•¼ vLLMì´ í•œ ê¸€ìì”© ì¤ë‹ˆë‹¤.
        temperature=0.7
    )

    # vLLMì—ì„œ ì˜¤ëŠ” ì¡°ê°(chunk)ì„ ë°›ìë§ˆì yieldë¡œ ë˜ì§
    async for chunk in stream:
        content = chunk.choices[0].delta.content
        if content:
            # ê·¸ëŒ€ë¡œ í…ìŠ¤íŠ¸ë§Œ ë³´ë‚¼ ìˆ˜ë„ ìˆê³ , SSE í¬ë§·ìœ¼ë¡œ ë³´ë‚¼ ìˆ˜ë„ ìˆìŒ
            yield content 

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    # 3. StreamingResponseë¡œ ê°ì‹¸ì„œ ë°˜í™˜
    return StreamingResponse(
        stream_generator(request.message, request.model),
        media_type="text/event-stream"  # ìŠ¤íŠ¸ë¦¬ë° í‘œì¤€ MIME íƒ€ì…
    )

######################################################################



api_router = APIRouter()
api_router.include_router(router=auth_router.router)
api_router.include_router(router=user_router.router)
api_router.include_router(router=post_router.router)
api_router.include_router(router=comment_router.router)
api_router.include_router(router=file_router.router)

app.include_router(api_router)